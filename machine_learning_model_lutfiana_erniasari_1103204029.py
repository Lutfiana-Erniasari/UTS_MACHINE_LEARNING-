# -*- coding: utf-8 -*-
"""Machine Learning Model_Lutfiana Erniasari_1103204029.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o7ZDbOUFfXmLJCqYY25ibG8yjJriqqWp
"""

# Library yang digunakan
from sklearn.datasets import load_breast_cancer
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split

#Load Data
data = load_breast_cancer()

#Show Core Information
print('Feature   :', data.feature_names)
print('Class     :', data.target_names)
print('Size Data :', len(data.target))

# Transform ke pandas Data Frame 
df = pd.DataFrame(data.data, columns = data.feature_names)
df['target'] = ['malignant' if target == 0 else 'benign' for target in data.target] # Tambahkan column target

# Tampilkan 5 data teratas
df.head()

# Pisahkan data masing - masing target
malignant = df[df['target'] == 'malignant']
benign = df[df['target'] == 'benign']

# Visualize banyaknya data tiap target
sns.barplot(x = ['malignant', 'benign'], y = [len(malignant), len(benign)])

plt.title('Banyaknya Data Tiap Target')
plt.show()

g = sns.PairGrid(df[['mean radius', 'worst radius', 'radius error', 'target']], hue="target", palette="Set2", hue_kws={"marker": ["o", "s"]})
g = g.map(plt.scatter, linewidths=1, edgecolor="w", s=40)
g = g.add_legend()

sns.pairplot(df[['mean texture', 'texture error', 'worst texture', 'target']], hue='target')

sns.pairplot(df[['mean fractal dimension', 'fractal dimension error', 'worst fractal dimension', 'target']], hue='target')

mean_column = list(df.columns[0:10])

plt.rcParams.update({'font.size': 8})
plot, graphs = plt.subplots(nrows=5, ncols=2, figsize=(12,14))
graphs = graphs.flatten()

for idx, graph in enumerate(graphs):
    graph.figure
      
    binwidth= (max(df[mean_column[idx]]) - min(df[mean_column[idx]]))/50
    bins = np.arange(min(df[mean_column[idx]]), max(df[mean_column[idx]]) + binwidth, binwidth)
    graph.hist([malignant[mean_column[idx]], benign[mean_column[idx]]], bins=bins, alpha=0.6, label=['Malignant','Benign'], color=['red','blue'])
    graph.legend(loc='upper right')
    graph.set_title(mean_column[idx])
plt.tight_layout()

error_column = list(df.columns[10:20])

plt.rcParams.update({'font.size': 8})
plot, graphs = plt.subplots(nrows=5, ncols=2, figsize=(12,14))
graphs = graphs.flatten()

for idx, graph in enumerate(graphs):
    graph.figure
      
    binwidth= (max(df[error_column[idx]]) - min(df[error_column[idx]]))/50
    bins = np.arange(min(df[error_column[idx]]), max(df[error_column[idx]]) + binwidth, binwidth)
    graph.hist([malignant[error_column[idx]], benign[error_column[idx]]], bins=bins, alpha=0.6, label=['Malignant','Benign'], color=['red','blue'])
    graph.legend(loc='upper right')
    graph.set_title(error_column[idx])
plt.tight_layout()

worst_column = list(df.columns[20:30])

plt.rcParams.update({'font.size': 8})
plot, graphs = plt.subplots(nrows=5, ncols=2, figsize=(12,14))
graphs = graphs.flatten()

for idx, graph in enumerate(graphs):
    graph.figure
      
    binwidth= (max(df[worst_column[idx]]) - min(df[worst_column[idx]]))/50
    bins = np.arange(min(df[worst_column[idx]]), max(df[worst_column[idx]]) + binwidth, binwidth)
    graph.hist([malignant[worst_column[idx]], benign[worst_column[idx]]], bins=bins, alpha=0.6, label=['Malignant','Benign'], color=['red','blue'])
    graph.legend(loc='upper right')
    graph.set_title(worst_column[idx])
plt.tight_layout()

# Pisahkan features dengan targets
x = df.iloc[:, 0:30] # features
y = df.iloc[:, -1] # targets

# Split data dengan ketentuan 60% train 40% test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4)

decision_tree = DecisionTreeClassifier(random_state = 0, max_depth = 2) # max_depth 2 paling optimal dari 3, 5, dan default
decision_tree = decision_tree.fit(x_train, y_train) # train dtree
tree.plot_tree(decision_tree) # plot dtree

# Hitung score akurasi
score = decision_tree.score(x_test, y_test)
print('Akurasi :',score)

random_forest = RandomForestClassifier(random_state = 0, max_depth = 4) # Optimal di max_depth 4 
random_forest.fit(x_train, y_train)

# Hitung score akurasi
score = random_forest.score(x_test, y_test)
print('Akurasi :',score)

svc = DecisionTreeClassifier(random_state = 0, max_depth = 2)
self_training = SelfTrainingClassifier(svc)
self_training.fit(x_train, y_train)

svc = DecisionTreeClassifier(random_state = 0, max_depth = 2)
self_training = SelfTrainingClassifier(svc)
self_training.fit(x_train, y_train)